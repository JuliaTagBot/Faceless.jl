{\rtf1\ansi\uc1\deff0\deflang1024
{\fonttbl{\f0\fnil\fcharset0 Times New Roman;}
{\f1\fnil\fcharset0 Arial;}
{\f2\fnil\fcharset0 Arial;}
{\f3\fnil\fcharset0 Courier New;}
{\f4\fnil\fcharset0 Zapf Chancery;}
{\f5\fnil\fcharset0 STIXGeneral;}
}
{\colortbl;
\red0\green0\blue0;
\red0\green0\blue255;
\red0\green255\blue255;
\red0\green255\blue0;
\red255\green0\blue255;
\red255\green0\blue0;
\red255\green255\blue0;
\red255\green255\blue255;
}
{\stylesheet
{\s0\qj\widctlpar\f0\fs20 \snext0 Normal;}
{\cs10 \additive\ssemihidden Default Paragraph Font;}
{\s1\qc\sb240\sa120\keepn\f0\b\fs40 \sbasedon0\snext0 Part;}
{\s2\ql\sb240\sa120\keepn\f0\b\fs40 \sbasedon0\snext0 heading 1;}
{\s3\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext0 heading 2;}
{\s4\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext0 heading 3;}
{\s5\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 4;}
{\s6\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 5;}
{\s7\ql\sb240\sa120\keepn\f0\b\fs24 \sbasedon0\snext0 heading 6;}
{\s8\qr\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext8 rightpar;}
{\s9\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext9 centerpar;}
{\s10\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext10 leftpar;}
{\s11\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equation;}
{\s12\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationNum;}
{\s13\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationAlign;}
{\s14\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationAlignNum;}
{\s15\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationArray;}
{\s16\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 equationArrayNum;}
{\s17\ql\sb120\sa120\keep\widctlpar\f0\fs20 \sbasedon0\snext0 theorem;}
{\s18\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 bitmapCenter;}
{\s20\qc\sb240\sa240\b\f0\fs36 \sbasedon0\snext21 Title;}
{\s21\qc\sa120\f0\fs20 \sbasedon0\snext0 author;}
{\s22\ql\tqc\tx4536\tqr\tx9072\f0\fs20 \sbasedon0\snext22 footer;}
{\s23\ql\tqc\tx4536\tqr\tx9072\f0\fs20 \sbasedon0\snext23 header;}
{\s30\ql\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 caption;}
{\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext0 Figure;}
{\s32\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext32 Table;}
{\s33\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext33 Tabular;}
{\s34\qc\sb120\sa0\keep\widctlpar\f0\fs20 \sbasedon0\snext34 Tabbing;}
{\s35\qj\li1024\ri1024\fi340\widctlpar\f0\fs20 \sbasedon0\snext35 Quote;}
{\s38\ql\widctlpar\f3\fs20 \snext38 verbatim;}
{\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext46 List;}
{\s47\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext47 List 1;}
{\s50\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 latex picture;}
{\s51\qc\sb120\sa120\keep\widctlpar\f0 \sbasedon0\snext0 subfigure;}
{\s61\ql\sb240\sa120\keepn\f0\b\fs32 \sbasedon0\snext62 bibheading;}
{\s62\ql\fi-567\li567\sb0\sa0\f0\fs20 \sbasedon0\snext62 bibitem;}
{\s64\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20 \sbasedon0\snext64 endnotes;}
{\s65\ql\fi-113\li397\lin397\f0\fs20 \sbasedon0\snext65 footnote text;}
{\s66\qj\fi-170\li454\lin454\f0\fs20 \sbasedon0\snext66 endnote text;}
{\cs62\super \additive\sbasedon10 footnote reference;}
{\cs63\super \additive\sbasedon10 endnote reference;}
{\s67\ql\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext67 acronym;}
{\s70\qc\sa120\b\f0\fs20 \sbasedon0\snext71 abstract title;}
{\s71\qj\li1024\ri1024\fi340\widctlpar\f0\fs20 \sbasedon0\snext0 abstract;}
{\s80\ql\sb240\sa120\keepn\f0\b\fs20 \sbasedon0\snext0 contents_heading;}
{\s81\ql\li425\tqr\tldot\tx8222\sb240\sa60\keepn\f0\fs20\b \sbasedon0\snext82 toc 1;}
{\s82\ql\li512\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext83 toc 2;}
{\s83\ql\li1024\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext84 toc 3;}
{\s84\ql\li1536\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext85 toc 4;}
{\s85\ql\li2048\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext86 toc 5;}
{\s86\ql\li2560\tqr\tldot\tx8222\sb60\sa60\keepn\f0\fs20 \sbasedon0\snext86 toc 6;}
}
{\info
{\title Original file was main.tex}
{\doccomm Created using latex2rtf 2.3.5 r1236 (released Jan 17 2014) on Sat Aug 29 21:17:12 2015
}
}
{\footer\pard\plain\f0\fs20\qc\chpgn\par}
\paperw12280\paperh15900\margl2680\margr2700\margt2540\margb1760\pgnstart0\widowctrl\qj\ftnbj\f0\aftnnar
{\pard\plain\s20\qc\sb240\sa240\b\f0\fs36\sl240\slmult1 \fi0 Using appearance for improving \par
\pard\plain\s20\qc\sb240\sa240\b\f0\fs36\sl240\slmult1 \fi0 facial expression recognition\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi300  {Andrei Zhabinski} { Belarussian State University \par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 of Informatics and Radioelectronics\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 Minsk, Belarus\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 Email: andrei.zhabinski@gmail.com} \par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi300  {Dzmitry Adzinets} { Belarussian State University \par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 of Informatics and Radioelectronics\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 Minsk, Belarus\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi0 Email: adzinets@bsuir.by}\par
\pard\plain\s21\qc\sa120\f0\fs20\sl240\slmult1 \fi300 \chdate \par
{\pard\plain\s70\qc\sa120\b\f0\fs20\sl240\slmult1 \fi300 Abstract\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \li1024\ri1024\fi300  Traditionally, recognition of a facial expression is performed by extracting and analysing specific set of landmarks (collectively often referred to as a "shape"). Here we take alternative approach, based not on shapes, but instead on appearance \endash  pixel values within region of interest. We construct classifier based on these data and compare its performance to a shape-based one. We also propose combined method using both sets of features. \par
}\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb360 \fi0 1  Introduction\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Understanding emotional state of a person plays key role in such areas as social and psychological research, marketing, gaming industry and many others. In most cases this information is transmitted between people via non-verbal channels. According to 
[{\field{\*\fldinst{\lang1024 REF BIB_mehrabian1968 \\* MERGEFORMAT }}{\fldrslt{mehrabian1968}}}
], about 55% of this information is generated by person\rquote s facial expression. Thus, creation of efficient algorithms for expression recognition is mandatory for advanced human-computer interaction. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Existing methods of facial expression recognition vary a lot. Some of them rely on static data (e.g. static images of a face), others - on dynamic ones (e.g. video), some represent input as optical flow, others - as connected vibrations or set of key points. And, of course, algorithms themselves differ a lot. Good overview of different approaches may be found in 
[{\field{\*\fldinst{\lang1024 REF BIB_stan2011 \\* MERGEFORMAT }}{\fldrslt{stan2011}}}
]. In this paper we will concentrate on analysis of static images of a face. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Almost all methods for analysing static images consist of two steps: feature extraction and recognition itself. Contents of these steps vary a lot, though. For example, on the first step there are 2 main groups of features: general and specific to faces. First group includes all features popular in computer vision. For instance, in 
[{\field{\*\fldinst{\lang1024 REF BIB_Littlewort04dynamicsof \\* MERGEFORMAT }}{\fldrslt{Littlewort04dynamicsof}}}
] Gabor filters are used, while 
[{\field{\*\fldinst{\lang1024 REF BIB_shana2009 \\* MERGEFORMAT }}{\fldrslt{shana2009}}}
] utilizes local binary patterns. Such techniques are easy to implement, but very often resulting features give low accuracy during recognition. Features, specific to faces, include, in particular, action units from FACS coding system 
[{\field{\*\fldinst{\lang1024 REF BIB_senechal2014 \\* MERGEFORMAT }}{\fldrslt{senechal2014}}}
] and sets of key points, describing shapes of main elements of a face. One method for obtaining key points, known as {\i active appearance models (AAM)} 
[{\field{\*\fldinst{\lang1024 REF BIB_cootes1998 \\* MERGEFORMAT }}{\fldrslt{cootes1998}}}
]
[{\field{\*\fldinst{\lang1024 REF BIB_Matthews03 \\* MERGEFORMAT }}{\fldrslt{Matthews03}}}
]
[{\field{\*\fldinst{\lang1024 REF BIB_raltiff2010 \\* MERGEFORMAT }}{\fldrslt{raltiff2010}}}
], gained a lot of attention in the area of facial analysis. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 With AAM, recognition is normally performed by extracting coordinates of key points and training classification algorithms on them. Such methods have pretty high accuracy, are intuitive and easily interpretable from psychological point of view. At the same time, this approach ignores information stored in {\i appearance} - pixel intensities within region of interest. In this paper, we evaluate relative importance of this information and propose combined method that uses both - location of key points and intensity of pixels in the region. \par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 {\*\bkmkstart BMalgo_shapetrain}2{\*\bkmkend BMalgo_shapetrain}  Shape-based method\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 In order to evaluate importance of appearance for facial expression recognition first of all we need to set up baseline to compare with. In this paper we use a variant of classic algorithm based on position of key points as such baseline. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Essentially, this algorithms consists of 2 steps: obtaining coordinates of key points and classification. Classification of objects by their feature vectors (coordinates of points in this case) is a standard task of supervised learning and is of no interest in the context. We only need to mention, that in this paper we used support vector machines for this task, since it demonstrated good results on similar problems.\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 On other hand, obtaining key points is much more difficult task. According to recent publications (e.g. 
[{\field{\*\fldinst{\lang1024 REF BIB_stan2011 \\* MERGEFORMAT }}{\fldrslt{stan2011}}}
]), it seems like the most popular algorithm for this is active appearance models. Here we describe in short essential steps of this algorithm. \par
{\pard\plain\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300  \par
\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0  \par
\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_annotated}1{\*\bkmkend BMfig_annotated}: Set of key points describing main elements of a face }{\field{\*\fldinst TC "1 Set of key points describing main elements of a face " \\f f}{\fldrslt }}\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300 AAM takes a set of images and corresponding sets of key points describing shape of main elements of a face (e.g. see figure {\field{\*\fldinst{\lang1024 REF BMfig_annotated \\* MERGEFORMAT }}{\fldrslt{?}}}). Using these data algorithm builds 2 statistical models: \par
{\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
{\i shape model} - parametric linear model describing possible variations of coordinates of key points. In this context, term "shape" denotes a vector of coordinates of key points: {{\field{\*\fldinst{ EQ {\i s}=({\i x}\\s\\do5({\fs16 1})\\,{\i y}\\s\\do5({\fs16 1})\\,{\i x}\\s\\do5({\fs16 2})\\,{\i y}\\s\\do5({\fs16 2})\\,...\\,{\i x}\\s\\do5({\fs16 {\i n}})\\,{\i y}\\s\\do5({\fs16 {\i n}}))}}{\fldrslt }}
}; \par
\pard\plain\s46\ql\fi-283\li283\lin283\sb0\sa120\widctlpar\tql\tx283\f0\fs20\sl240\slmult1 \sb50 \li600\fi-300 \bullet\tab
{\i appearance model} - similar model, but describing possible variations of pixel intensities in the area of interest. Also, unlike shape model, term "appearance" here denotes not a vector, but an image {{\i A}({\i x})} defined over all pixels {{\i x}{\f5\u8712*}{\i s}}. \par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi300 Note, that here we borrow somewhat inexact, but convenient notation from 
[{\field{\*\fldinst{\lang1024 REF BIB_Matthews03 \\* MERGEFORMAT }}{\fldrslt{Matthews03}}}
] and write {{\i x}{\f5\u8712*}{\i s}} to refer to all pixels inside shape and not to shape elements themselves. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Since different images may contain different number of pixels inside shape, all images are first aligned to some mean shape. Normally, piecewise affine transformation is used for this: first, set of key points is triangulated, and then every triangle is warped to new coordinates via regular affine transformation (see image 2). Piecewise affine transformation is essential to this work, since it will be also used in appearance-based and (implicitly) in combined methods. \par
{\pard\plain\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300  \par
\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0  \par
\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_pwa}2{\*\bkmkend BMfig_pwa}: Example of piecewise affine transformation. On the left: original image and corresponding triangulation. In the middle: target image. On the right: original image warped to the shape of the target one }{\field{\*\fldinst TC "2 Example of piecewise affine transformation. On the left: original image and corresponding triangulation. In the middle: target image. On the right: original image warped to the shape of the target one " \\f f}{\fldrslt }}\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300 When applying to new images, algorithm first finds position of a face and applies approximate shape to it. Then, shape and appearance models are iteratively fitted to an image to find exact location of key points. Detailed description of active appearance models can be found in 
[{\field{\*\fldinst{\lang1024 REF BIB_cootes1998 \\* MERGEFORMAT }}{\fldrslt{cootes1998}}}
] and 
[{\field{\*\fldinst{\lang1024 REF BIB_Matthews03 \\* MERGEFORMAT }}{\fldrslt{Matthews03}}}
]. \par
{\pard\plain\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300  \par
\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0  \par
\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_aamfitting}3{\*\bkmkend BMfig_aamfitting}: AAM fitting. On the left: initial guess for a shape. On the right: fitted shape }{\field{\*\fldinst TC "3 AAM fitting. On the left: initial guess for a shape. On the right: fitted shape " \\f f}{\fldrslt }}\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300 Putting it all together we get the following 2 stage algorithm.\par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 First, we train AAM on a dataset to use it on later steps. We don\rquote t dive deeper into details of AAM training and fitting here because it\rquote s not the main topic of this paper, but instead simply state existence of functions {{\i a}{\i a}{\i m}{\i t}{\i r}{\i a}{\i i}{\i n}} and {{\i a}{\i a}{\i m}{\i f}{\i i}{\i t}}. Also it\rquote s important to note that, though in our experiments we used the same dataset everywhere, dataset for training AAMs should not necessarily be the same as for training main classifier. In fact, it\rquote s totally valid to have pre-trained AAM model or even dataset with precomputed coordinates of key points. In our case we obtain these coordinates by fitting trained AAM to every image in the dataset. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Finally, we pass these coordinates to SVM training procedure to get instance of the classifier. \par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Predicting expression for new image is straightforward: first we obtain that image, then fit AAM to find coordinates of key points and then simply apply trained SVM to get prediction. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Note, that in this method appearance is used purely for obtaining coordinates of key points, but it isn\rquote t used on classification step. On the contrary, in next section we will describe method that primarily uses appearance data. \par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 {\*\bkmkstart BMalgo_apptrain}3{\*\bkmkend BMalgo_apptrain}  Appearance-based method\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Possibility to learn facial expression from a shape is well known and described in a number of papers. But at the same time this possibility with regard to appearance got from little to no interest in computer vision community. So first of all we need some evidence that appearance holds at least some information about encoded facial expression. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 To prove it we propose following informal experiment. We take several images of faces and transform them to the mean shape computed previously using AAMs. We use same piecewise affine transformation which gives us pretty smooth deformation. Result of this transformation is depicted at image {\field{\*\fldinst{\lang1024 REF BMfig_imgtomeanshape \\* MERGEFORMAT }}{\fldrslt{?}}}. \par
{\pard\plain\s31\qc\sb120\sa0\keep\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300  \par
\pard\plain\s9\qc\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0  \par
\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 {Figure {\*\bkmkstart BMfig_imgtomeanshape}4{\*\bkmkend BMfig_imgtomeanshape}: Transforming images to the mean shape. On the left: images of emotional faces. On the right: same faces transformed to the mean shape. Though shape is the same, facial expressions are still easily distinguishable }{\field{\*\fldinst TC "4 Transforming images to the mean shape. On the left: images of emotional faces. On the right: same faces transformed to the mean shape. Though shape is the same, facial expressions are still easily distinguishable " \\f f}{\fldrslt }}\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300 From pictures it\rquote s clear that all key points for eyes, eyebrows, nose and mouth have identical coordinates between all images, i.e. all transformed faces have {\i exactly the same shape}. But it\rquote s also noticeable that original facial expressions are still easily guessable. This provides us with evidence that appearance really contains information that encodes human facial expression, and possibly quite a lot of it. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 To compare amount of information stored in appearance with one stored in shape, we build another classifier, this time based on pixel intensities. \par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 First we preprocess all images in a dataset: fit pre-trained AAM to each of them to obtain shape of a face and then use this shape to warp image to AAM\rquote s mean shape. This way all images get same shape (and thus same number of pixels inside), but keep most information about appearance. Then we flatten images, i.e. rearrange pixels inside shape into a single vector of a fixed size. Mathematically, {{\i f}{\i l}} denotes a mapping from matrix {{\field{\*\fldinst{ EQ {\i A}{\f5\u8712*}{\i R}\\s\\up5({\fs16 {\i m}{\u215*}{\i n}})}}{\fldrslt }}
} to a vector {{\field{\*\fldinst{ EQ {\i a}{\f5\u8712*}{\i R}\\s\\up5({\fs16 {\i k}})}}{\fldrslt }}
}. Finally, we use these flattened appearance vectors to train SVM classifier. \par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 During application of classifier to a new image, we follow same steps as during training. First, we apply AAM to obtain shape of a face on the image. Then, using this shape, we warp new image to the mean shape, making it usable for classifier. Finally, we apply trained SVM model to pixels of the transformed image to get prediction. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Before moving further, let\rquote s briefly recall common and different parts of two algorithms. Both of them are split into 2 stages - training and predicting. Both require preprocessing to obtain features. First algorithm uses coordinates of key points as its features, while second is based on pixel intensities. In both cases SVM is used as a final classifier. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 We should also note, that in this specific work we use AAM for aligning image to the mean shape, though, it\rquote s not mandatory precondition. For example, in 
[{\field{\*\fldinst{\lang1024 REF BIB_Taigman2014 \\* MERGEFORMAT }}{\fldrslt{Taigman2014}}}
] face is located using several predefined landmarks and then 3D-modeled to obtain possible transformations.\par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 {\*\bkmkstart BMalgo_combtrain}4{\*\bkmkend BMalgo_combtrain}  Combined method\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 Having 2 similar methods with different sets of features it\rquote s natural to combine them in a single algorithm: we can merge information about key point coordinates and pixel intensities into a single vector and use it for classification. Since all essential parts have already been discussed in previous sections, here we will simply list steps of combined algorithm. \par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 Algorithm is very similar to previous 2 except for step (6) where we combine 2 kinds of features, i.e. simply concatenate their vectors (operator {[;]} here has signature {{\field{\*\fldinst{ EQ [;]:({\i R}\\s\\up5({\fs16 {\i m}})\\,{\i R}\\s\\up5({\fs16 {\i n}})){\u8614*}{\i R}\\s\\up5({\fs16 {\i m}+{\i n}})}}{\fldrslt }}
}).\par
{\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 \qc [Sorry. Ignored {\plain\f3\\begin\{algorithm\} ... \\end\{algorithm\}}]\par
}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 In the following section we discuss our experiments and results of all 3 described methods.\par
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 5  Experiments and results\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 We used Extended Cohn-Kanade dataset (CK+)
[{\field{\*\fldinst{\lang1024 REF BIB_ck_ \\* MERGEFORMAT }}{\fldrslt{ck\\s\\do5({\fs16 })}}}
] to evaluate our algorithms. This dataset was specifically collected to support research in facial expression tracking and recognition. Currently, it consists of 327 sequences of video frames (from neutral expression to a strongly pronounced emotion) totalling in 10708 images. Each image comes with corresponding shape file, containing coordinates of key points, and every sequence additionally has label of expressed emotion. CK+ uses set of 6 basic emotions proposed by Paul Ekman (e.g. see 
[{\field{\*\fldinst{\lang1024 REF BIB_ekman2005 \\* MERGEFORMAT }}{\fldrslt{ekman2005}}}
]) and considered the de facto standard for their recognition. These emotions are: happiness, sadness, surprise, fear, disgust and anger. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 For training AAM we used AAMToolbox 
[{\field{\*\fldinst{\lang1024 REF BIB_aamtoolbox \\* MERGEFORMAT }}{\fldrslt{aamtoolbox}}}
], which is freely available for research purposes. Using 4-core CPU Intel i7 and CK+ dataset training AAM took about 3.5 hours. Training SVM classifier on 327 labeled images (only last image from each sequence \endash  the one with maximally expressed emotion \endash  was used) took about 7 seconds. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 These timings, however, cover only training stage: during fitting AAM can locate key points within 200 ms, while SVM gives prediction in about 25 ms, which makes it possible to uses our algorithms in near-real time systems. \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi300 For evaluating results we used standard method of cross validation. After 10 experiments we got following results: \par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \fi0 {\par
\pard\plain\s32\qc\sb120\sa0\keep\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi0  \par
\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 {Table 1: Accuracy of 3 classifiers}{\field{\*\fldinst TC "1 Accuracy of 3 classifiers" \\f t}{\fldrslt }}\par
{{\pard\plain\s30\ql\sb120\sa120\keep\widctlpar\f0\sl240\slmult1 \fi0 \par
{\trowd\clbrdrt\brdrs\clbrdrb\brdrdb\clbrdrr\brdrs\cellx5581\clbrdrl\brdrs\clbrdrt\brdrs\clbrdrb\brdrdb\cellx6899
{\pard\intbl\qr { algorithm}\cell}
{\pard\intbl\qc {accuracy}\cell}
\row}
{\trowd\clbrdrr\brdrs\cellx5581\clbrdrl\brdrs\cellx6899
{\pard\intbl\qr {  {shape-based}}\cell}
{\pard\intbl\qc {89.4%}\cell}
\row}
{\trowd\clbrdrr\brdrs\cellx5581\clbrdrl\brdrs\cellx6899
{\pard\intbl\qr {{appearance-based}}\cell}
{\pard\intbl\qc {86.3%}\cell}
\row}
{\trowd\clbrdrb\brdrs\clbrdrr\brdrs\cellx5581\clbrdrl\brdrs\clbrdrb\brdrs\cellx6899
{\pard\intbl\qr {{combined}}\cell}
{\pard\intbl\qc {93.5%}\cell}
\row}
} \par
}}\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb240 \fi300 As we can see, appearance-based algorithm was able to achieve results pretty close to those of shaped-based one. This means, in particular, that appearance holds pretty large amount of information about facial expression. Also, we see that combined classifier outperformed both of these separate algorithms, which proves that appearance doesn\rquote t simply reflect shape variations, but rather adds some amount of unique information. \par
\column
\pard\plain\s3\ql\sb240\sa120\keepn\f0\b\fs32\sl240\slmult1 \sb240 \fi0 6  Conclusion and discussion\par
\pard\plain\s0\qj\widctlpar\f0\fs20\sl240\slmult1 \sb60 \fi0 This paper describes and evaluates 3 methods for facial expression recognition, based on shape, appearance and their combination. Results suggest that information about appearance, previously mostly ignored, holds valuable amount of additional information that can be used to significantly improve classification performance. Although here we used raw pixel intensities as appearance features, more sophisticated options are available. In particular, recent progress in deep learning allows to obtain better representation for appearance features. This will become basis for future work. \par
}}
