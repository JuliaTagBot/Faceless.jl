\documentclass[conference]{IEEEtran}

\usepackage[pdftex]{graphicx}
\graphicspath{{./pdf/}{./images/}}

% \usepackage[justification=centering]{caption}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother


\begin{document}

\title{Hybrid differentiation of tensors \\ for machine learning}


\author{
\IEEEauthorblockN{Andrei Zhabinski}
\IEEEauthorblockA{
Belarussian State University \\of Informatics and Radioelectronics\\
Minsk, Belarus\\
Email: andrei.zhabinski@gmail.com}
\and
\IEEEauthorblockN{Dzmitry Adzinets}
\IEEEauthorblockA{
Belarussian State University \\of Informatics and Radioelectronics\\
Minsk, Belarus\\
Email: adzinets@bsuir.by}}

\maketitle


\begin{abstract}
%\boldmath
TODO


\end{abstract}

% no keywords

\IEEEpeerreviewmaketitle


\section{Introduction}

A significant portion of machine learning (ML) algorithms directly relies on gradient-based optimization methods. As their name states, these methods require computing a gradient of a loss function on each step of optimization. Simple models like logistic regression have well-known formulas for computing partial derivatives. However, recent progress in machine learning and especially deep neural networks has given the rise to much more complicated models and loss functions. (examples/references?) 

Manually computing gradients of such functions is time-consuming and error-prone, so often computer-based methods are used instead. Such methods fall into several categories, and each category has their advantages and downsides. In this paper we present a method that falls in-between 2 of these categories (hence, hybrid method) and is specifically designed with a focus on machine learning applications. One important difference from other approaches is that our method also supports efficient calculation of derivatives of functions from $R^m$ to $R^n$ where both $m$ and $n$ are large, whereas other methods either require one of them to be small (add references? ForwardDiff.jl, ReverseDiffSource.jl), or can't handle vector-functions at all (add references? Mathematica, SymPy). 

The rest of this paper is structured as follow. In the next section we revisit major categories (families?) of computer-based differentiation algorithms and explain where our method falls in. In section 3 we describe actual method as applied to scalars (numbers) and lower-order tensors, while in section 4 we extend it to higher-order tensors using  Einstein notation. 


\section{Overview of computer-based differentiation methods}

To the best of our knowledge, most computer-based differentiation methods are divided into 3 main categories: 

\begin{enumerate}
\item Numeric (also known as finite difference methods)
\item Symbolic
\item Automatic
\end{enumerate}


\textbf{Numeric differentiation (ND)} methods are the simplest ones and directly rely on the definition of derivative. We say that $\frac{df}{dx}$ is a derivative of a function $f(x)$ with respect to $x$ if: 

$$\frac{df}{dx} = \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}$$

Given this definition we can calculate derivative of any function by computing it at $x$ and $x + \Delta x$ where $\Delta x$ is some little constant, and then dividing the result by that constant. 

The major advantage of this method is that it's able to compute derivatives of any function differentiable at both - $x$ and $\Delta x$. The main disadvantage comes from complexity of choosing good $\Delta x$: too small values may lead to round-off errors during floating point operations, while too large values lead to weak approximation of real value of a derivative. Attempts have been done to imrpove precision using multiple differences (e.g. \cite{fornberg1988}), but in general case accuracy of numeric differentiation stays unstable. 

\textbf{Symbolic differentiation (SD)} takes a different approach - instead of evaluating a function, it constructs a symbolic expression representing its derivative. Technically, symbolic differentiaton relies on a fact that algebraic expressions are either primitive (e.g. summation, product, exponent, etc.) or a combination of the above. For primitive expressions there's already a well-known set of rules, i.e. $\frac{d}{dx}cos(x) = sin(x)$. Combined expressions are handled using a chain rule, i.e. if $z = z(y)$ is a function of $y$ which in turn is a function $y = y(x)$ of $x$, then derivative $\frac{dz}{dx}$ may be expressed as:

$$\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}$$

Chain rule and differentiation rules for primitive expressions make it possible to divide any algebraic expression into primitive parts and find their derivatives one by one (ugly sentence, rephrase!). (make a note that this part is important since our method also relies on these 2 statements). Here we demonstrate this method by example. 

Let's say we want to differentiate expression $f(x) = sin(x^2)$. Since there are only 2 calls, we can introduce one intermediate function $g(x)$ so that $f(x) = sin(g(x))$. Using chain rule from above we get: 

$$\frac{df}{dx} = \frac{df}{dg}\frac{dg}{dx} = \frac{d(sin(g))}{dg}\frac{d(x^2)}{dx}$$

From primitive rules we know that $\frac{d}{dg}sin(g) = cos(g)$ and $\frac{d}{dx}x^2 = 2x$. Replacing $g$ with its original value and multiplying two derivatives we get: 

$$\frac{df}{dx} = cos(x^2) \times 2x$$

Which is correct derivative of the specified function. 

Unlike numeric approach, symbolic differentiation produces code for \textit{exact} calculation of derivatives. Also, in many cases this code is as efficient as possible and close to what a human expert would derive. Moreover, produced symbolic expression may then be further optimized (e.g. to fit memory requirements or improve numeric stability) or even translated to GPU or any other computation engine, which is extremely desirebale property in the context of neural networks and related models.

On the other hand, symbolic differentiation has much more limited applicability. First of all, since the goal of this family of methods is to produce symbolic expression of a derivative, it is limited only to functions continuously differentiable in all points. In computer program, however, most conditions and loops introduce discontinuity, so they are normally not allowed. Another restriction of symbolic differentiation is that most practical implementations (including highly adopted \cite{SymPy} and \cite{Mathematica}), are essentially univariate, i.e. they don't support vector inputs and are thus unapplicable to most machine learning functions.

\textbf{Automatic differentiation (AD)} is a set of techniques to numerically evaluate the derivative of a function. Similarly to symbolic differentiation, it decomposes algebraic expressions into a set of primitive subexpressions and then uses chain rule to combine results. Unlike symbolic approach, however, AD doesn't produce new expression, but only computes derivative at a point, so it can easily handle conditions and loops. AD has 2 main modes: forward and reverse.

\textit{Forward-mode AD} consists of a single pass from input arguments to the value of a function, during which both - value of the function and its derivative - are calculated. Typically, this mode is implemented using dual numbers - special extension of real numbers with additional part that holds the derivative of a variable in question. However, this method has pretty bad complexity for function $R^m \to R^n$ where $m \gg n$, i.e. most loss machines in machine learning. Thus we don't dive deeper in description of this method and guide interested reader to \cite{stan}.

\textit{Reverse-mode AD} consists of 2 passes. On the \textit{forward} pass it computes values of all primitive expressions and records them into intermediate variables on a "tape". On the \textit{reverse} pass, the method starts with the derivative of output variable by itself (i.e. always $1$) and traverses expression tree backward to calculate derivatives of the output w.r.t. each of intermediate variables. Here's an example of such computation. 

Let $z = sin(x^2)$. This complex expression contains 2 primitive expressions, which we immediately write onto a tape:

\begin{enumerate}
\item $y = x^2$
\item $z = sin(y)$
\end{enumerate}

By convention, we also replace input and output variables by new indexed names to make all variables look similar. This way we get:

\begin{enumerate}
\item $w_1 = x$
\item $w_2 = w_1^2$
\item $w_3 = sin(w_2)$
\end{enumerate}

We seek to find derivative $\frac{dz}{dx} = \frac{dw_3}{dw_1}$. We start with the fact that derivative the output variable by itself is $\frac{dw_3}{dw_3} = 1$ (the proof is trivially inferred from the definition of derivative). 

From the chain rule we know that $\frac{dw_3}{dw_2} = \frac{dw_3}{dw_3} \times \frac{dw_3}{dw_2}$. We already know that $\frac{dw_3}{dw_3} = 1$ and from primitive rules we also know that $\frac{d(sin(w_2))}{dw_2} = cos(w_2)$, so multiplying these parts we get $\frac{dw_3}{dw_2} = 1 \times cos(w_2) = cos(w_2)$. Note, however, that AD doesn't ever create or transform symbolic expressions, but just calculates actual values of intermediate variables and their derivatives. 

Reverse-mode AD has gained particular popularity in the field of machine learning and is implemented in such frameworks as Theano \cite{Theano} and TensorFlow \cite{TensorFlow}, as well as in dedicated libraries such as AutoGrad \cite{AutoGrad}. Strengths of this method include efficient (in terms of processing time) calculation of derivative of functions $R^m \to R^n$ where $m \gg n$ and transparent handling of multivariate vectors. Downsides of AD include high memory usage (because of the need to keep all variables on a tape during both - forward and reverse pass) and lack of symbolic representation that could be used for further optimizations and code generation. 

There's also a few methods that try to combine strengths of symbolic and automatic differentiation. One notable example is \textbf{D*} algorithm from \cite{guenter2007}. In their paper, Guenter et al. describe a library in C++ that overloads common algebraic operations for their \textit{V(ariable)} class to produce expression graph, and then aggressively optimize this graph to eliminate repeating blocks. \textbf{D*} allows for vector-valued functions where each output component may represent an independent function of input arguments. This gives flexibility in defining vector-valued functions, but in worst case (i.e. when there are no repeating blocks to be eliminated) may result in a run time proportional to the number of output values. 

Our method is similar in spirit to \textbf{D*}, but instead of supporting independent functions, we limit output components to be functions of the same basic structure and only different indices (e.g. $z_i = f(x_j, y_k)$). This is much more common in machine learning and enables generating more compact code with much smaller run time. Moreover, our method works with functions that output not only vectors, but also matrices and, in general, tensors of any rank. 

In the next section we describe our method in a way that works for scalars and partially for lower-order tensors (i.e. vectors and matrices), and in section 4 we extend this framework to support higher-order tensors.

\section{Hybrid method for scalars and lower-order tensors}

The ultimate goal of our method is to be able to produce symbolic derivatives for algebraic functions commonly used in machine learning. Since we seek for a symbolic representation, we limit our framework to continiously differentiable function, i.e. we don't support conditional operators and loops as they may, in general case, introduce points of discontinuity. To our mind, this is not a hard limitation in the context of ML since most loss functions that use loops can be transformed into a form with summation. We do, however, support operators like $min$, $max$, $sign$ and similar that may be thought of as conditional, but don't create discontinuity points.

In general, our method is based on approach used in reverse-mode AD, but instead of computing \textit{values} of variables and derivatives, we compose their \textit{symbolic expressions} to build a computational graph suitable for further optimization. For demonstration purposes, let's consider expression $z = x_1x_2 + sin(x_1)$ and find derivatives $\frac{dz}{dx_1}$ and $\frac{dz}{dx_2}$.

Forward pass in our case consists of 2 stages: recording all primitive operations onto a tape and evaluating "example values" (which we explain a bit later). Recording operations includes parsing subexpressions into intermediate variables and constructing a list of primitive expressions, e.g. for expression above such a list would look like this:

\begin{enumerate}
\item $w_1 = x_1$
\item $w_2 = x_2$
\item $w_3 = w_1 \times w_2$
\item $w_4 = sin(w_1)$
\item $w_5 = w_3 + w4$ (same as $z$)
\end{enumerate}

This expression list, however, lacks type information needed to distinguish between scalars and higher-rank tensors. To obtain this information, we introduce a notion of "example values" - values attached to each variable and having the same type as real values could have. E.g. we can use value 1 for $x_1$ and $x_2$ to indicate that input parameters are scalars, or random 2x2 matrices to indicate that they are instances of tensor-2. Note, that real values passed to generated expression later don't need to have the same value or size (for tensors), but only the same type. 

Having example values of input parameters we evaluate each primitive expression to determine their own example values and find out derivative rules based on it (move note about differentiation rules somewhere above).


\section{Hybrid method for higher-order tensors}

TODO

\section{Implementation in Julia}

0. Rationale: symbolic programming and overloaded operations, accepted in math
1. Expression graph
2. Operator overloading

\newpage

\section{Conclusion and discussion}



% conference papers do not normally have an appendix


\bibliography{references} 
\bibliographystyle{ieeetr}



% that's all folks
\end{document}
